{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sklearn' from '/usr/local/lib/python3.4/dist-packages/sklearn/__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.optimize\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "from importlib import reload\n",
    "%matplotlib inline\n",
    "reload(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sample_patches import *\n",
    "from display_layer import *\n",
    "from gradient import *\n",
    "from autoencoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    check_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.60395210289e-08\n"
     ]
    }
   ],
   "source": [
    "hidden_size = np.array([6, 3, 6])\n",
    "visible_size = 12\n",
    "rho = 0.2\n",
    "beta = 3.0\n",
    "lambda_ = 1e-4\n",
    "N = 10\n",
    "data = np.random.uniform(0.1, 0.9, (N, visible_size))\n",
    "theta = initialize(hidden_size, visible_size)\n",
    "grad = autoencoder_loss(theta, visible_size, hidden_size, lambda_, rho, beta, data)[1]\n",
    "J = lambda theta: autoencoder_loss(theta, visible_size, hidden_size, lambda_, rho, beta, data)[0]\n",
    "G = compute_gradient(J, theta)\n",
    "print(np.max(np.abs(G - grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_patches(num=10000, size=8):\n",
    "    patches = np.zeros((num, size * size * 3))\n",
    "    x = 0\n",
    "    step = max(1, (num + 4) // 5)\n",
    "    for i in range(0, num, step):\n",
    "        x += 1\n",
    "        f = open('./data2.7/X' + str(x) + '.pk', 'rb')\n",
    "        d = pickle.load(f)\n",
    "        patches[i:i + step] = sample_patches(d, step, size)\n",
    "        del d\n",
    "        f.close()\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patches = gen_patches(10000, 8)\n",
    "display_layer(patches[:100], 'pathes.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = np.array([75])\n",
    "visible_size = 192\n",
    "params = [{'rho': 0.06,\n",
    "           'lambda_': 1e-4,\n",
    "           'beta': 3.0},\n",
    "          {'rho': 0.01,\n",
    "           'lambda_': 1e-4,\n",
    "           'beta': 3.0},\n",
    "          {'rho': 0.12,\n",
    "           'lambda_': 1e-4,\n",
    "           'beta': 3.0},\n",
    "          {'rho': 0.06,\n",
    "           'lambda_': 1e-3,\n",
    "           'beta': 3.0},\n",
    "          {'rho': 0.06,\n",
    "           'lambda_': 1e-5,\n",
    "           'beta': 3.0},\n",
    "          {'rho': 0.06,\n",
    "           'lambda_': 1e-4,\n",
    "           'beta': 6.0},\n",
    "          {'rho': 0.06,\n",
    "           'lambda_': 1e-4,\n",
    "           'beta': 0.5},\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 lambda=0.0001 rho=0.06 beta=3\n",
      "115.28576964946446 0.2055247589413744\n",
      "#1 lambda=0.0001 rho=0.01 beta=3\n",
      "150.16876525275555 0.6469779141336756\n",
      "#2 lambda=0.0001 rho=0.12 beta=3\n",
      "87.17816049809541 0.14512721566040104\n",
      "#3 lambda=0.001 rho=0.06 beta=3\n",
      "114.11653950454014 0.5493606604189617\n",
      "#4 lambda=1e-05 rho=0.06 beta=3\n",
      "110.84767702785535 0.09518917914640546\n",
      "#5 lambda=0.0001 rho=0.06 beta=6\n",
      "225.88150107756556 0.20885916391694728\n",
      "#6 lambda=0.0001 rho=0.06 beta=0.5\n",
      "23.41827171199798 0.19989232746085617\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(params)):    \n",
    "    rho = params[i]['rho']\n",
    "    lambda_ = params[i]['lambda_']\n",
    "    beta = params[i]['beta']\n",
    "    print('#{0} lambda={1:.2g} rho={2:.2g} beta={3:.2g}'.format(i, lambda_, rho, beta))\n",
    "    loss = lambda theta: autoencoder_loss(theta, visible_size, hidden_size, lambda_, rho, beta, patches)\n",
    "    theta = initialize(hidden_size, visible_size)\n",
    "    opt_res = scipy.optimize.minimize(loss, theta, method='L-BFGS-B', jac=True, options={'disp': True,\n",
    "                                                                                         'maxiter': 2000\n",
    "                                                                                        })\n",
    "    print(loss(theta)[0], opt_res['fun'])\n",
    "    W, b = get_params(opt_res['x'], visible_size, hidden_size)\n",
    "    display_layer(W[0].T, 'filters_' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = np.array([75])\n",
    "visible_size = 192\n",
    "rho = 0.06\n",
    "lambda_ = 1e-4\n",
    "beta = 3.0\n",
    "loss = lambda theta: autoencoder_loss(theta, visible_size, hidden_size, lambda_, rho, beta, patches)\n",
    "theta = initialize(hidden_size, visible_size)\n",
    "opt_res = scipy.optimize.minimize(loss, theta, method='L-BFGS-B', jac=True, options={'disp': True,\n",
    "                                                                                      'maxiter': 2000\n",
    "                                                                                    })\n",
    "theta = opt_res['x']\n",
    "np.savez('theta.npz', theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_classifiers(gen_features, test_batch_size=500):\n",
    "    f = open('./data2.7/train.pk', 'rb')\n",
    "    train = pickle.load(f)\n",
    "    f.close()\n",
    "    t1 = time.clock()\n",
    "    features = gen_features(train['X'])\n",
    "    t1 = time.clock() - t1\n",
    "    print('Train fearutes generated. {0:.2g}s.'.format(t1))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    t1  = time.clock()\n",
    "    rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500, max_depth=30, n_jobs=3)\n",
    "    rf.fit(features, train['y'].ravel())\n",
    "    t1 = time.clock() - t1\n",
    "    print('RF trained. {0:.2g}s.'.format(t1))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    t1 = time.clock()\n",
    "    logr = sklearn.linear_model.LogisticRegression(solver='sag', max_iter=100)\n",
    "    logr.fit(features, train['y'].ravel())\n",
    "    t1 = time.clock() - t1\n",
    "    print('Logistic regression trained. {0:.2g}s.'.format(t1))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    del features    \n",
    "    del train\n",
    "    \n",
    "    f = open('./data2.7/test.pk', 'rb')\n",
    "    test = pickle.load(f)\n",
    "    f.close()\n",
    "    N = test['X'].shape[0]\n",
    "    rf_score = 0.0\n",
    "    logr_score = 0.0\n",
    "    for i in range(0, N, test_batch_size):\n",
    "        print('Test batch #{0}'.format(i // test_batch_size))\n",
    "        sys.stdout.flush()\n",
    "        bs = min(test_batch_size, N-i)\n",
    "        features = gen_features(test['X'][i:i+bs])\n",
    "        y = test['y'][i:i+bs].ravel()\n",
    "        rf_score += np.sum(rf.predict(features) == y)\n",
    "        logr_score += np.sum(logr.predict(features) == y)\n",
    "    rf_score /= N\n",
    "    logr_score /= N\n",
    "    print('Random forest score: {0:.4f}'.format(rf_score))\n",
    "    print('Logistic regression score: {0:.4f}'.format(logr_score))    \n",
    "    sys.stdout.flush()\n",
    "    del features\n",
    "    del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pixel_features(data):\n",
    "    return data\n",
    "def autoencoder_features(data, theta, hidden_size, visible_size, step=8):\n",
    "    N = data.shape[0]\n",
    "    D = int(math.sqrt(data.shape[1] // 3))\n",
    "    layer_num = hidden_size.shape[0] // 2 + 1\n",
    "    data = data.reshape(N, D, D, 3)\n",
    "    features = []\n",
    "    for i in range(0, D-7, step):\n",
    "        for j in range(0, D-7, step):\n",
    "            features.append(autoencoder_transform(theta, visible_size, hidden_size, \n",
    "                                                  layer_num, data[:, i:i+8, j:j+8, :].reshape(N, 8*8*3)))\n",
    "    return np.hstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fearutes generated. 7e-06s.\n",
      "RF trained. 8.2e+02s.\n",
      "Logistic regression trained. 1.9e+03s.\n",
      "Test batch #0\n",
      "Test batch #1\n",
      "Test batch #2\n",
      "Test batch #3\n",
      "Test batch #4\n",
      "Test batch #5\n",
      "Test batch #6\n",
      "Test batch #7\n",
      "Test batch #8\n",
      "Test batch #9\n",
      "Test batch #10\n",
      "Test batch #11\n",
      "Test batch #12\n",
      "Test batch #13\n",
      "Test batch #14\n",
      "Test batch #15\n",
      "Random forest score: 0.4376\n",
      "Logistic regression score: 0.3401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "run_classifiers(pixel_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fearutes generated. 9.8s.\n",
      "RF trained. 1.7e+02s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timgaripov/work/prac/semestr5/05/autoencoder.py:119: RuntimeWarning: overflow encountered in exp\n",
      "  Y = 1.0 / (1.0 + np.exp(-Y))\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression trained. 7.9e+02s.\n",
      "Test batch #0\n",
      "Test batch #1\n",
      "Test batch #2\n",
      "Test batch #3\n",
      "Test batch #4\n",
      "Test batch #5\n",
      "Test batch #6\n",
      "Test batch #7\n",
      "Test batch #8\n",
      "Test batch #9\n",
      "Test batch #10\n",
      "Test batch #11\n",
      "Test batch #12\n",
      "Test batch #13\n",
      "Test batch #14\n",
      "Test batch #15\n",
      "Random forest score: 0.4241\n",
      "Logistic regression score: 0.4756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timgaripov/work/prac/semestr5/05/autoencoder.py:119: RuntimeWarning: overflow encountered in exp\n",
      "  Y = 1.0 / (1.0 + np.exp(-Y))\n"
     ]
    }
   ],
   "source": [
    "run_classifiers(lambda data: autoencoder_features(data, theta, hidden_size, visible_size, step=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fearutes generated. 38s.\n",
      "RF trained. 2.6e+02s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timgaripov/work/prac/semestr5/05/autoencoder.py:119: RuntimeWarning: overflow encountered in exp\n",
      "  Y = 1.0 / (1.0 + np.exp(-Y))\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression trained. 3e+03s.\n",
      "Test batch #0\n",
      "Test batch #1\n",
      "Test batch #2\n",
      "Test batch #3\n",
      "Test batch #4\n",
      "Test batch #5\n",
      "Test batch #6\n",
      "Test batch #7\n",
      "Test batch #8\n",
      "Test batch #9\n",
      "Test batch #10\n",
      "Test batch #11\n",
      "Test batch #12\n",
      "Test batch #13\n",
      "Test batch #14\n",
      "Test batch #15\n",
      "Random forest score: 0.4343\n",
      "Logistic regression score: 0.5058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timgaripov/work/prac/semestr5/05/autoencoder.py:119: RuntimeWarning: overflow encountered in exp\n",
      "  Y = 1.0 / (1.0 + np.exp(-Y))\n"
     ]
    }
   ],
   "source": [
    "run_classifiers(lambda data: autoencoder_features(data, theta, hidden_size, visible_size, step=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = np.array([100, 64, 100])\n",
    "visible_size = 192\n",
    "rho = 0.05\n",
    "lambda_ = 1e-6\n",
    "beta = 0.5\n",
    "loss = lambda theta: autoencoder_loss(theta, visible_size, hidden_size, lambda_, rho, beta, patches)\n",
    "theta = initialize(hidden_size, visible_size)\n",
    "opt_res = scipy.optimize.minimize(loss, theta, method='L-BFGS-B', jac=True, options={'disp': True,\n",
    "                                                                                      'maxiter': 2000\n",
    "                                                                                    })\n",
    "theta = opt_res['x']\n",
    "np.savez('theta_3.npz', theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fearutes generated. 18s.\n",
      "RF trained. 6.5e+02s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timgaripov/work/prac/semestr5/05/autoencoder.py:119: RuntimeWarning: overflow encountered in exp\n",
      "  Y = 1.0 / (1.0 + np.exp(-Y))\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression trained. 6.4e+02s.\n",
      "Test batch #0\n",
      "Test batch #1\n",
      "Test batch #2\n",
      "Test batch #3\n",
      "Test batch #4\n",
      "Test batch #5\n",
      "Test batch #6\n",
      "Test batch #7\n",
      "Test batch #8\n",
      "Test batch #9\n",
      "Test batch #10\n",
      "Test batch #11\n",
      "Test batch #12\n",
      "Test batch #13\n",
      "Test batch #14\n",
      "Test batch #15\n",
      "Random forest score: 0.4002\n",
      "Logistic regression score: 0.4128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timgaripov/work/prac/semestr5/05/autoencoder.py:119: RuntimeWarning: overflow encountered in exp\n",
      "  Y = 1.0 / (1.0 + np.exp(-Y))\n"
     ]
    }
   ],
   "source": [
    "run_classifiers(lambda data: autoencoder_features(data, theta, hidden_size, visible_size, step=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W, b = get_params(theta, visible_size, hidden_size)\n",
    "display_layer(W[0].T, 'check.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
